#!/usr/bin/env python3
"""
ðŸ”— Entity Correlation Engine - Advanced Cross-Platform Analysis
LakyLuk OSINT Investigation Suite

Features:
âœ… Cross-platform profile correlation and identity matching
âœ… AI-powered similarity analysis with multi-dimensional scoring
âœ… Behavioral pattern recognition across social networks
âœ… Advanced entity relationship mapping and clustering
âœ… Temporal analysis of identity evolution across platforms
âœ… Confidence scoring with uncertainty quantification

AI Integration:
- Multi-model ensemble for identity correlation
- Deep learning similarity embeddings
- Natural language processing for bio/description analysis
- Computer vision for profile picture matching
- Graph neural networks for relationship analysis
"""

import asyncio
import json
import hashlib
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple, Set
from dataclasses import dataclass
from collections import defaultdict
import re
import math

import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import DBSCAN
import networkx as nx

from ..core.enhanced_orchestrator import EnhancedInvestigationOrchestrator
from ..utils.similarity_calculator import SimilarityCalculator
from ..utils.confidence_estimator import ConfidenceEstimator


@dataclass
class EntityProfile:
    """Unified entity profile across platforms"""
    platform: str
    username: str
    profile_url: str
    display_name: Optional[str] = None
    bio: Optional[str] = None
    location: Optional[str] = None
    profile_picture_url: Optional[str] = None
    follower_count: Optional[int] = None
    following_count: Optional[int] = None
    post_count: Optional[int] = None
    verified: bool = False
    created_date: Optional[datetime] = None
    last_activity: Optional[datetime] = None
    content_samples: List[str] = None
    metadata: Dict[str, Any] = None

    def __post_init__(self):
        if self.content_samples is None:
            self.content_samples = []
        if self.metadata is None:
            self.metadata = {}


@dataclass
class CorrelationResult:
    """Result of entity correlation analysis"""
    entity_id: str
    profiles: List[EntityProfile]
    similarity_score: float
    confidence_score: float
    correlation_factors: Dict[str, float]
    supporting_evidence: List[str]
    conflicting_evidence: List[str]
    ai_analysis: Dict[str, Any]
    created_at: datetime


class EntityCorrelationEngine:
    """Advanced cross-platform entity correlation system"""

    def __init__(self, ai_orchestrator: EnhancedInvestigationOrchestrator = None):
        self.ai_orchestrator = ai_orchestrator
        self.similarity_calculator = SimilarityCalculator()
        self.confidence_estimator = ConfidenceEstimator()

        # Correlation thresholds
        self.correlation_thresholds = {
            'high_confidence': 0.85,
            'medium_confidence': 0.65,
            'low_confidence': 0.45,
            'minimum_threshold': 0.30
        }

        # Feature weights for correlation scoring
        self.feature_weights = {
            'name_similarity': 0.25,
            'bio_similarity': 0.20,
            'location_similarity': 0.15,
            'profile_picture_similarity': 0.15,
            'content_similarity': 0.15,
            'temporal_patterns': 0.10
        }

        # Platform-specific correlation factors
        self.platform_factors = {
            'facebook': {'reliability': 0.9, 'data_richness': 0.8},
            'instagram': {'reliability': 0.8, 'data_richness': 0.7},
            'linkedin': {'reliability': 0.95, 'data_richness': 0.9},
            'twitter': {'reliability': 0.7, 'data_richness': 0.6}
        }

        # Caching for expensive operations
        self.correlation_cache = {}
        self.embedding_cache = {}

    async def correlate_entities(self, profiles: List[EntityProfile]) -> List[CorrelationResult]:
        """
        Correlate entities across platforms using AI-enhanced analysis

        Args:
            profiles: List of entity profiles from different platforms

        Returns:
            List of correlation results with grouped entities
        """
        try:
            if len(profiles) < 2:
                return []

            # Precompute features for all profiles
            profile_features = await self._extract_profile_features(profiles)

            # Calculate pairwise similarities
            similarity_matrix = await self._calculate_similarity_matrix(profile_features)

            # Cluster profiles using advanced algorithms
            clusters = await self._cluster_profiles(profiles, similarity_matrix)

            # Generate correlation results for each cluster
            correlation_results = []

            for cluster_profiles in clusters:
                if len(cluster_profiles) > 1:
                    correlation_result = await self._generate_correlation_result(
                        cluster_profiles, profile_features
                    )
                    correlation_results.append(correlation_result)

            # AI enhancement of correlation results
            if self.ai_orchestrator:
                enhanced_results = await self._ai_enhance_correlations(correlation_results)
                correlation_results = enhanced_results

            # Sort by confidence score
            correlation_results.sort(key=lambda x: x.confidence_score, reverse=True)

            return correlation_results

        except Exception as e:
            print(f"Entity correlation error: {e}")
            return []

    async def find_cross_platform_matches(self, target_profile: EntityProfile,
                                        candidate_profiles: List[EntityProfile],
                                        threshold: float = None) -> List[Tuple[EntityProfile, float]]:
        """
        Find cross-platform matches for a specific target profile

        Args:
            target_profile: Profile to find matches for
            candidate_profiles: Profiles to search through
            threshold: Minimum similarity threshold

        Returns:
            List of matching profiles with similarity scores
        """
        if threshold is None:
            threshold = self.correlation_thresholds['minimum_threshold']

        matches = []

        try:
            # Extract features for target profile
            target_features = await self._extract_single_profile_features(target_profile)

            for candidate in candidate_profiles:
                if candidate.platform == target_profile.platform:
                    continue  # Skip same platform

                # Extract candidate features
                candidate_features = await self._extract_single_profile_features(candidate)

                # Calculate comprehensive similarity
                similarity_score = await self._calculate_profile_similarity(
                    target_features, candidate_features
                )

                if similarity_score >= threshold:
                    matches.append((candidate, similarity_score))

            # AI validation of matches
            if self.ai_orchestrator and matches:
                validated_matches = await self._ai_validate_matches(target_profile, matches)
                matches = validated_matches

            # Sort by similarity score
            matches.sort(key=lambda x: x[1], reverse=True)

            return matches

        except Exception as e:
            print(f"Cross-platform matching error: {e}")
            return []

    async def analyze_identity_evolution(self, correlated_profiles: List[EntityProfile]) -> Dict[str, Any]:
        """
        Analyze how an identity evolves across platforms and time

        Args:
            correlated_profiles: Profiles believed to belong to same entity

        Returns:
            Identity evolution analysis with temporal insights
        """
        try:
            evolution_analysis = {
                'temporal_progression': [],
                'platform_adaptation': {},
                'identity_consistency': {},
                'behavioral_changes': {},
                'content_evolution': {},
                'network_evolution': {}
            }

            # Sort profiles by creation date
            sorted_profiles = sorted(
                [p for p in correlated_profiles if p.created_date],
                key=lambda x: x.created_date
            )

            # Analyze temporal progression
            for i, profile in enumerate(sorted_profiles):
                temporal_point = {
                    'timestamp': profile.created_date.isoformat(),
                    'platform': profile.platform,
                    'username': profile.username,
                    'identity_markers': await self._extract_identity_markers(profile)
                }

                if i > 0:
                    # Compare with previous profile
                    prev_profile = sorted_profiles[i-1]
                    changes = await self._analyze_identity_changes(prev_profile, profile)
                    temporal_point['changes_from_previous'] = changes

                evolution_analysis['temporal_progression'].append(temporal_point)

            # Platform adaptation analysis
            evolution_analysis['platform_adaptation'] = await self._analyze_platform_adaptation(
                correlated_profiles
            )

            # Identity consistency scoring
            evolution_analysis['identity_consistency'] = await self._score_identity_consistency(
                correlated_profiles
            )

            # AI-powered behavioral analysis
            if self.ai_orchestrator:
                ai_evolution_insights = await self.ai_orchestrator.analyze_identity_evolution(
                    correlated_profiles
                )
                evolution_analysis['ai_insights'] = ai_evolution_insights

            return evolution_analysis

        except Exception as e:
            print(f"Identity evolution analysis error: {e}")
            return {}

    async def detect_fake_profiles(self, profiles: List[EntityProfile]) -> Dict[str, Any]:
        """
        Detect potentially fake or fraudulent profiles using AI analysis

        Args:
            profiles: Profiles to analyze for authenticity

        Returns:
            Fake profile detection results with risk scores
        """
        try:
            detection_results = {
                'analyzed_profiles': len(profiles),
                'fake_indicators': {},
                'risk_scores': {},
                'authenticity_assessment': {},
                'suspicious_patterns': []
            }

            for profile in profiles:
                profile_id = f"{profile.platform}:{profile.username}"

                # Analyze various fake profile indicators
                indicators = await self._analyze_fake_indicators(profile)
                detection_results['fake_indicators'][profile_id] = indicators

                # Calculate risk score
                risk_score = await self._calculate_fake_risk_score(profile, indicators)
                detection_results['risk_scores'][profile_id] = risk_score

                # AI authenticity assessment
                if self.ai_orchestrator:
                    authenticity = await self.ai_orchestrator.assess_profile_authenticity(profile)
                    detection_results['authenticity_assessment'][profile_id] = authenticity

            # Detect suspicious cross-platform patterns
            detection_results['suspicious_patterns'] = await self._detect_suspicious_patterns(profiles)

            return detection_results

        except Exception as e:
            print(f"Fake profile detection error: {e}")
            return {}

    # Internal helper methods

    async def _extract_profile_features(self, profiles: List[EntityProfile]) -> Dict[str, Dict]:
        """Extract comprehensive features from all profiles"""
        features = {}

        for profile in profiles:
            profile_id = f"{profile.platform}:{profile.username}"
            features[profile_id] = await self._extract_single_profile_features(profile)

        return features

    async def _extract_single_profile_features(self, profile: EntityProfile) -> Dict[str, Any]:
        """Extract features from a single profile"""
        features = {
            'profile': profile,
            'name_tokens': self._tokenize_name(profile.display_name or profile.username),
            'bio_vector': await self._vectorize_text(profile.bio or ""),
            'location_normalized': self._normalize_location(profile.location or ""),
            'content_vector': await self._vectorize_content(profile.content_samples),
            'temporal_features': self._extract_temporal_features(profile),
            'numeric_features': self._extract_numeric_features(profile),
            'linguistic_features': await self._extract_linguistic_features(profile)
        }

        # AI-enhanced feature extraction
        if self.ai_orchestrator:
            ai_features = await self.ai_orchestrator.extract_profile_features(profile)
            features['ai_features'] = ai_features

        return features

    async def _calculate_similarity_matrix(self, profile_features: Dict[str, Dict]) -> np.ndarray:
        """Calculate pairwise similarity matrix for all profiles"""
        profile_ids = list(profile_features.keys())
        n_profiles = len(profile_ids)
        similarity_matrix = np.zeros((n_profiles, n_profiles))

        for i in range(n_profiles):
            for j in range(i + 1, n_profiles):
                similarity = await self._calculate_profile_similarity(
                    profile_features[profile_ids[i]],
                    profile_features[profile_ids[j]]
                )
                similarity_matrix[i, j] = similarity
                similarity_matrix[j, i] = similarity

        return similarity_matrix

    async def _calculate_profile_similarity(self, features1: Dict, features2: Dict) -> float:
        """Calculate comprehensive similarity between two profiles"""
        similarities = {}

        # Name similarity
        similarities['name'] = self.similarity_calculator.calculate_name_similarity(
            features1['name_tokens'], features2['name_tokens']
        )

        # Bio similarity
        if features1['bio_vector'].size > 0 and features2['bio_vector'].size > 0:
            similarities['bio'] = cosine_similarity(
                features1['bio_vector'].reshape(1, -1),
                features2['bio_vector'].reshape(1, -1)
            )[0, 0]
        else:
            similarities['bio'] = 0.0

        # Location similarity
        similarities['location'] = self.similarity_calculator.calculate_location_similarity(
            features1['location_normalized'], features2['location_normalized']
        )

        # Content similarity
        if features1['content_vector'].size > 0 and features2['content_vector'].size > 0:
            similarities['content'] = cosine_similarity(
                features1['content_vector'].reshape(1, -1),
                features2['content_vector'].reshape(1, -1)
            )[0, 0]
        else:
            similarities['content'] = 0.0

        # Temporal pattern similarity
        similarities['temporal'] = self.similarity_calculator.calculate_temporal_similarity(
            features1['temporal_features'], features2['temporal_features']
        )

        # AI-enhanced similarity
        if self.ai_orchestrator:
            ai_similarity = await self.ai_orchestrator.calculate_advanced_similarity(
                features1, features2
            )
            similarities['ai_enhanced'] = ai_similarity
            self.feature_weights['ai_enhanced'] = 0.15

        # Weighted average
        total_similarity = sum(
            similarities.get(feature.replace('_similarity', ''), 0) * weight
            for feature, weight in self.feature_weights.items()
            if feature.replace('_similarity', '') in similarities
        )

        return min(max(total_similarity, 0.0), 1.0)

    async def _cluster_profiles(self, profiles: List[EntityProfile],
                              similarity_matrix: np.ndarray) -> List[List[EntityProfile]]:
        """Cluster profiles using advanced clustering algorithms"""
        try:
            # Convert similarity to distance matrix
            distance_matrix = 1 - similarity_matrix

            # Use DBSCAN for clustering
            clustering = DBSCAN(
                metric='precomputed',
                eps=1 - self.correlation_thresholds['minimum_threshold'],
                min_samples=2
            )

            cluster_labels = clustering.fit_predict(distance_matrix)

            # Group profiles by cluster
            clusters = defaultdict(list)
            for i, label in enumerate(cluster_labels):
                if label != -1:  # -1 means noise/outlier
                    clusters[label].append(profiles[i])

            return list(clusters.values())

        except Exception as e:
            print(f"Clustering error: {e}")
            # Fallback to simple similarity-based grouping
            return await self._simple_similarity_clustering(profiles, similarity_matrix)

    async def _simple_similarity_clustering(self, profiles: List[EntityProfile],
                                          similarity_matrix: np.ndarray) -> List[List[EntityProfile]]:
        """Simple fallback clustering method"""
        clusters = []
        used_indices = set()

        for i in range(len(profiles)):
            if i in used_indices:
                continue

            cluster = [profiles[i]]
            used_indices.add(i)

            for j in range(i + 1, len(profiles)):
                if j not in used_indices and similarity_matrix[i, j] >= self.correlation_thresholds['minimum_threshold']:
                    cluster.append(profiles[j])
                    used_indices.add(j)

            if len(cluster) > 1:
                clusters.append(cluster)

        return clusters

    async def _generate_correlation_result(self, cluster_profiles: List[EntityProfile],
                                         profile_features: Dict[str, Dict]) -> CorrelationResult:
        """Generate comprehensive correlation result for a cluster"""
        # Generate unique entity ID
        entity_id = self._generate_entity_id(cluster_profiles)

        # Calculate overall similarity score
        similarity_scores = []
        for i in range(len(cluster_profiles)):
            for j in range(i + 1, len(cluster_profiles)):
                profile1_id = f"{cluster_profiles[i].platform}:{cluster_profiles[i].username}"
                profile2_id = f"{cluster_profiles[j].platform}:{cluster_profiles[j].username}"

                if profile1_id in profile_features and profile2_id in profile_features:
                    similarity = await self._calculate_profile_similarity(
                        profile_features[profile1_id], profile_features[profile2_id]
                    )
                    similarity_scores.append(similarity)

        avg_similarity = np.mean(similarity_scores) if similarity_scores else 0.0

        # Calculate confidence score
        confidence_score = self.confidence_estimator.calculate_correlation_confidence(
            cluster_profiles, similarity_scores
        )

        # Extract correlation factors
        correlation_factors = await self._extract_correlation_factors(cluster_profiles)

        # Generate supporting and conflicting evidence
        supporting_evidence, conflicting_evidence = await self._analyze_evidence(cluster_profiles)

        # AI analysis
        ai_analysis = {}
        if self.ai_orchestrator:
            ai_analysis = await self.ai_orchestrator.analyze_entity_correlation(cluster_profiles)

        return CorrelationResult(
            entity_id=entity_id,
            profiles=cluster_profiles,
            similarity_score=avg_similarity,
            confidence_score=confidence_score,
            correlation_factors=correlation_factors,
            supporting_evidence=supporting_evidence,
            conflicting_evidence=conflicting_evidence,
            ai_analysis=ai_analysis,
            created_at=datetime.now()
        )

    def _generate_entity_id(self, profiles: List[EntityProfile]) -> str:
        """Generate unique entity ID based on profile characteristics"""
        # Create deterministic ID based on profile features
        id_components = []
        for profile in sorted(profiles, key=lambda x: x.platform):
            component = f"{profile.platform}:{profile.username}"
            id_components.append(component)

        id_string = "|".join(id_components)
        return hashlib.md5(id_string.encode()).hexdigest()[:16]

    # Additional utility methods for text processing and feature extraction

    def _tokenize_name(self, name: str) -> List[str]:
        """Tokenize and normalize name for comparison"""
        if not name:
            return []

        # Clean and tokenize
        name = re.sub(r'[^\w\s]', '', name.lower())
        tokens = name.split()

        # Remove common stopwords
        stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of'}
        tokens = [token for token in tokens if token not in stopwords]

        return tokens

    async def _vectorize_text(self, text: str) -> np.ndarray:
        """Convert text to TF-IDF vector"""
        if not text or len(text.strip()) == 0:
            return np.array([])

        try:
            # Use cached vectorizer or create new one
            cache_key = f"vectorizer_{hash(text)}"
            if cache_key in self.embedding_cache:
                return self.embedding_cache[cache_key]

            vectorizer = TfidfVectorizer(max_features=100, stop_words='english')
            vector = vectorizer.fit_transform([text]).toarray()[0]

            self.embedding_cache[cache_key] = vector
            return vector

        except Exception:
            return np.array([])

    async def _vectorize_content(self, content_samples: List[str]) -> np.ndarray:
        """Vectorize content samples for similarity analysis"""
        if not content_samples:
            return np.array([])

        combined_content = " ".join(content_samples[:10])  # Limit to avoid memory issues
        return await self._vectorize_text(combined_content)

    def _normalize_location(self, location: str) -> str:
        """Normalize location string for comparison"""
        if not location:
            return ""

        # Basic location normalization
        location = location.lower().strip()

        # Remove common location prefixes/suffixes
        location = re.sub(r'\b(city|town|state|country|area|region)\b', '', location)
        location = re.sub(r'\s+', ' ', location).strip()

        return location

    def _extract_temporal_features(self, profile: EntityProfile) -> Dict[str, Any]:
        """Extract temporal features from profile"""
        features = {}

        if profile.created_date:
            features['creation_timestamp'] = profile.created_date.timestamp()
            features['account_age_days'] = (datetime.now() - profile.created_date).days

        if profile.last_activity:
            features['last_activity_timestamp'] = profile.last_activity.timestamp()
            features['days_since_activity'] = (datetime.now() - profile.last_activity).days

        return features

    def _extract_numeric_features(self, profile: EntityProfile) -> Dict[str, float]:
        """Extract numeric features for analysis"""
        features = {}

        features['follower_count'] = float(profile.follower_count or 0)
        features['following_count'] = float(profile.following_count or 0)
        features['post_count'] = float(profile.post_count or 0)

        # Calculate ratios
        if profile.follower_count and profile.following_count:
            features['follower_following_ratio'] = profile.follower_count / profile.following_count

        features['verified'] = float(profile.verified)

        return features

    async def _extract_linguistic_features(self, profile: EntityProfile) -> Dict[str, Any]:
        """Extract linguistic features from profile text"""
        features = {}

        text_data = []
        if profile.bio:
            text_data.append(profile.bio)
        if profile.content_samples:
            text_data.extend(profile.content_samples[:5])

        if text_data:
            combined_text = " ".join(text_data)

            # Basic linguistic features
            features['text_length'] = len(combined_text)
            features['word_count'] = len(combined_text.split())
            features['avg_word_length'] = np.mean([len(word) for word in combined_text.split()])
            features['exclamation_count'] = combined_text.count('!')
            features['question_count'] = combined_text.count('?')
            features['hashtag_count'] = combined_text.count('#')
            features['mention_count'] = combined_text.count('@')

        return features

    async def _extract_correlation_factors(self, profiles: List[EntityProfile]) -> Dict[str, float]:
        """Extract factors that support correlation"""
        factors = {}

        # Platform diversity factor
        platforms = set(p.platform for p in profiles)
        factors['platform_diversity'] = len(platforms) / len(profiles)

        # Username similarity factor
        usernames = [p.username for p in profiles]
        factors['username_similarity'] = self.similarity_calculator.calculate_username_cluster_similarity(usernames)

        # Temporal consistency factor
        creation_dates = [p.created_date for p in profiles if p.created_date]
        if len(creation_dates) > 1:
            factors['temporal_consistency'] = self.similarity_calculator.calculate_temporal_consistency(creation_dates)

        return factors

    async def _analyze_evidence(self, profiles: List[EntityProfile]) -> Tuple[List[str], List[str]]:
        """Analyze supporting and conflicting evidence for correlation"""
        supporting_evidence = []
        conflicting_evidence = []

        # Check for consistent information
        names = [p.display_name for p in profiles if p.display_name]
        locations = [p.location for p in profiles if p.location]

        if len(set(names)) == 1 and len(names) > 1:
            supporting_evidence.append(f"Consistent display name across {len(names)} profiles")
        elif len(set(names)) > 1:
            conflicting_evidence.append(f"Different display names: {', '.join(set(names))}")

        if len(set(locations)) == 1 and len(locations) > 1:
            supporting_evidence.append(f"Consistent location across {len(locations)} profiles")

        # Check for temporal patterns
        creation_dates = [p.created_date for p in profiles if p.created_date]
        if len(creation_dates) > 1:
            date_range = max(creation_dates) - min(creation_dates)
            if date_range.days < 30:
                supporting_evidence.append("Profiles created within 30 days of each other")
            elif date_range.days > 365:
                conflicting_evidence.append("Large time gap between profile creations")

        return supporting_evidence, conflicting_evidence